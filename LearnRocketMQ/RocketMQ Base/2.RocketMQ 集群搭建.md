记录 RocketMQ 学习过程中集群搭建过程，方便后续复习。包含 NameServer 集群和 Broker 集群，使用 Docker 搭建，方便快捷。

# 1.集群简介
## 1.1 架构
![](https://raw.githubusercontent.com/lujiahao0708/PicRepo/master/blogPic/RocketMQ/RocketMQ%E5%9F%BA%E7%A1%80/RocketMQ%E6%9E%B6%E6%9E%84%E5%9B%BE.png)

## 1.2 角色介绍
* NameServer：管理Broker
* Broker：暂存和传输消息
* Producer：消息的发送者
* Consumer：消息接收者
* Topic：区分消息的种类；一个发送者可以发送消息给一个或者多个Topic；一个消息的接收者可以订阅一个或者多个Topic消息
* Message Queue：相当于是Topic的分区；用于并行发送和接收消息


## 1.3 集群模式
### 单 Master 模式
严格意义上说单节点模式不能称为集群，同时这种方式风险较大，一旦 Broker 节点重启或者宕机，会导致整个服务不可用。不建议线上环境使用，可以用于本地测试。

### 多 Master 模式
集群中节点均为 Master 节点，无 Slave节点。该模式优点是配置简单，某一 Master 节点宕机不会影响其他节点；然而缺点是在节点宕机期间，该节点未被消费的消息在节点恢复前不能被订阅消费，消息实时性会受到影响。

### 多 Master 多 Slave 异步刷盘模式
每个 Master 节点都配置一个 Slave节点，集群中存在多对 Master-Slave，主从节点间同步数据采用`异步复制方式`，主备有短暂消息延迟（毫秒级）。这种模式的优缺点如下：

- 优点：即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，同时 Master 宕机后，消费者仍然可以从 Slave 消费，而且此过程对应用透明，无需人工干预，性能同多Master模式几乎一样；
- 缺点就是当 Master 宕机，磁盘损坏情况下会丢失少量消息。

### 多 Master 多 Slave 同步刷盘模式
与上面的异步模式节点分配相同，仅仅是在主从节点间同步数据采用`同步双写方式`，即只有主从节点都写成功，才向应用返回成功，这种模式的优缺点如下：

- 优点：数据与服务都无单点故障，消息无延迟，服务可用性与数据可用性都非常高；
- 缺点：性能比异步复制模式略低（大约低10%左右），发送单个消息的 RT 会略高，且目前版本在主节点宕机后，备机不能自动切换为主机。

# 2.集群工作原理
1. 首先启动 NameServer 集群。NameServer 是一个几乎无状态节点，在整个 RocketMQ 集群中作为一个路由控制中心，监听端口等待 Broker、Producer 和 Consumer 连接。

2. Broker 节点类型分为 Master 和 Slave，一个 Master 可以对应多个 Slave，但是一个 Slave 只能对应一个 Master。Master 与 Slave 的对应关系通过指定相同的BrokerName，不同的 BrokerId 来定义，BrokerId 为 0 表示 Master，非 0 表示 Slave。Broker 集群启动后，每个 Broker 节点会跟所有的 NameServer 节点保持长连接，定时发送心跳包。心跳包中包含当前 Broker 节点信息(IP 和端口等)以及存储所有 Topic 信息。注册成功后，NameServer 集群中会保存 Topic 和 Broker 的映射关系。

3. 收发消息前，先创建 Topic，创建 Topic 时需要指定该 Topic 要存储在哪些 Broker 上，也可以在发送消息时自动创建 Topic。

4. Producer 发送消息，启动时先跟 NameServer 集群中的其中一个节点（随机选择）建立长连接，并从 NameServer 中获取 Topic 路由信息（即当前发送的 Topic 存在哪些 Broker 上），轮询从队列列表中选择一个队列，然后与队列所在的 Broker 建立长连接从而向 Broker 发消息。且定时向 Master 发送心跳。Producer 完全无状态。

5. Consumer 跟 Producer 类似，跟其中一个节点（随机选择）建立长连接，定期从 NameServer 取 Topic 路由信息（即当前订阅 Topic 存在哪些 Broker 上），然后直接跟 Broker 建立连接通道，开始消费消息。Consumer 既可以从 Broker Master 节点订阅消息，也可以从 Slave 节点订阅消息，订阅规则由 Broker 配置决定。


# 3.集群搭建
本实例搭建的 2m-2s（双主双从同步双写）模式集群。

## 3.1 docker-compose.yml
```
version: '3.5'
services:
  rmqnamesrv-a:
    image: apacherocketmq/rocketmq:4.5.0
    container_name: rmqnamesrv-a
    ports:
      - 9876:9876
    volumes:
      - ./data/namesrv/rmqnamesrv-a/logs:/home/rocketmq/logs
    command: sh mqnamesrv
    networks:
      - rmq
  rmqnamesrv-b:
    image: apacherocketmq/rocketmq:4.5.0
    container_name: rmqnamesrv-b
    ports:
      - 9877:9876
    volumes:
      - ./data/namesrv/rmqnamesrv-b/logs:/home/rocketmq/logs
    command: sh mqnamesrv
    networks:
      - rmq

  rmqbroker-a:
    image: apacherocketmq/rocketmq:4.5.0
    container_name: rmqbroker-a
    links:
      - rmqnamesrv-a
      - rmqnamesrv-b
    ports:
      - 10909:10909
      - 10911:10911
      - 10912:10912
    environment:
      TZ: Asia/Shanghai
      JAVA_OPT_EXT: "-server -Xms256m -Xmx256m -Xmn256m"
    volumes:
      - ./data/broker/rmqbroker-a/logs:/home/rocketmq/logs
      - ./data/broker/rmqbroker-a/store:/home/rocketmq/store
      - ./conf/broker-a.conf:/opt/rocketmq-4.5.0/conf/broker.conf
    command: sh mqbroker -c /opt/rocketmq-4.5.0/conf/broker.conf
    networks:
      - rmq
  rmqbroker-a-s:
    image: apacherocketmq/rocketmq:4.5.0
    container_name: rmqbroker-a-s
    links:
      - rmqnamesrv-a
      - rmqnamesrv-b
    ports:
      - 10913:10909
      - 10914:10911
      - 10915:10912
    environment:
      TZ: Asia/Shanghai
      JAVA_OPT_EXT: "-server -Xms256m -Xmx256m -Xmn256m"
    volumes:
      - ./data/broker/rmqbroker-a-s/logs:/home/rocketmq/logs
      - ./data/broker/rmqbroker-a-s/store:/home/rocketmq/store
      - ./conf/broker-a-s.conf:/opt/rocketmq-4.5.0/conf/broker.conf
    command: sh mqbroker -c /opt/rocketmq-4.5.0/conf/broker.conf
    networks:
      - rmq
  
  rmqbroker-b:
    image: apacherocketmq/rocketmq:4.5.0
    container_name: rmqbroker-b
    links:
      - rmqnamesrv-a
      - rmqnamesrv-b
    ports:
      - 10919:10909
      - 10920:10911
      - 10921:10912
    environment:
      TZ: Asia/Shanghai
      JAVA_OPT_EXT: "-server -Xms256m -Xmx256m -Xmn256m"
    volumes:
      - ./data/broker/rmqbroker-b/logs:/home/rocketmq/logs
      - ./data/broker/rmqbroker-b/store:/home/rocketmq/store
      - ./conf/broker-b.conf:/opt/rocketmq-4.5.0/conf/broker.conf
    command: sh mqbroker -c /opt/rocketmq-4.5.0/conf/broker.conf
    networks:
      - rmq
  rmqbroker-b-s:
    image: apacherocketmq/rocketmq:4.5.0
    container_name: rmqbroker-b-s
    links:
      - rmqnamesrv-a
      - rmqnamesrv-b
    ports:
      - 10922:10909
      - 10923:10911
      - 10924:10912
    environment:
      TZ: Asia/Shanghai
      JAVA_OPT_EXT: "-server -Xms256m -Xmx256m -Xmn256m"
    volumes:
      - ./data/broker/rmqbroker-b-s/logs:/home/rocketmq/logs
      - ./data/broker/rmqbroker-b-s/store:/home/rocketmq/store
      - ./conf/broker-b-s.conf:/opt/rocketmq-4.5.0/conf/broker.conf
    command: sh mqbroker -c /opt/rocketmq-4.5.0/conf/broker.conf
    networks:
      - rmq

  rmqconsole:
    image: styletang/rocketmq-console-ng
    container_name: rmqconsole
    ports:
      - 8080:8080
    environment:
      # 注意这里 namesrv 地址要填对
      JAVA_OPTS: "-Drocketmq.namesrv.addr=rmqnamesrv-a:9876;rmqnamesrv-b:9876 -Dcom.rocketmq.sendMessageWithVIPChannel=false"
    depends_on:
      - rmqnamesrv-a
      - rmqnamesrv-b
    networks:
      - rmq

networks:
  rmq:
    name: rmq
    driver: bridge
```

## 3.2 配置文件
采用的是双主双从模式，因此会有 4 个配置文件。两个主节点配置文件名分别为 `broker-a.conf` 和 `broker-b.conf`，从节点配置文件为 `broker-a-s.conf` 和 `broker-b-s.conf`。
> 篇幅问题仅展示关键配置，详细配置文件参看：https://github.com/lujiahao0708/LearnSeries/tree/master/LearnDocker/RocketMQ/RocketMQ-cluster-2m-2s-sync/conf

### broker-a.conf
```
#所属集群名字
brokerClusterName=rocketmq-cluster
#broker名字，注意此处不同的配置文件填写的不一样
brokerName=broker-a
#0 表示 Master，>0 表示 Slave
brokerId=0
#nameServer地址，分号分割
namesrvAddr=rmqnamesrv-a:9876;rmqnamesrv-b:9876
#Broker 的角色
#- ASYNC_MASTER 异步复制Master
#- SYNC_MASTER 同步双写Master
#- SLAVE
brokerRole=SYNC_MASTER
#刷盘方式
#- ASYNC_FLUSH 异步刷盘
#- SYNC_FLUSH 同步刷盘
flushDiskType=SYNC_FLUSH
```
### broker-a-s.conf
```
#所属集群名字
brokerClusterName=rocketmq-cluster
#broker名字，注意此处不同的配置文件填写的不一样
brokerName=broker-a
#0 表示 Master，>0 表示 Slave
brokerId=1
#nameServer地址，分号分割
namesrvAddr=rmqnamesrv-a:9876;rmqnamesrv-b:9876
#Broker 的角色
#- ASYNC_MASTER 异步复制Master
#- SYNC_MASTER 同步双写Master
#- SLAVE
brokerRole=SLAVE
#刷盘方式
#- ASYNC_FLUSH 异步刷盘
#- SYNC_FLUSH 同步刷盘
flushDiskType=ASYNC_FLUSH
```
### broker-b.conf
```
#所属集群名字
brokerClusterName=rocketmq-cluster
#broker名字，注意此处不同的配置文件填写的不一样
brokerName=broker-b
#0 表示 Master，>0 表示 Slave
brokerId=0
#nameServer地址，分号分割
namesrvAddr=rmqnamesrv-a:9876;rmqnamesrv-b:9876
#Broker 的角色
#- ASYNC_MASTER 异步复制Master
#- SYNC_MASTER 同步双写Master
#- SLAVE
brokerRole=SYNC_MASTER
#刷盘方式
#- ASYNC_FLUSH 异步刷盘
#- SYNC_FLUSH 同步刷盘
flushDiskType=SYNC_FLUSH
```
### broker-b-s.conf
```
#所属集群名字
brokerClusterName=rocketmq-cluster
#broker名字，注意此处不同的配置文件填写的不一样
brokerName=broker-b
#0 表示 Master，>0 表示 Slave
brokerId=1
#nameServer地址，分号分割
namesrvAddr=rmqnamesrv-a:9876;rmqnamesrv-b:9876
#Broker 的角色
#- ASYNC_MASTER 异步复制Master
#- SYNC_MASTER 同步双写Master
#- SLAVE
brokerRole=SLAVE
#刷盘方式
#- ASYNC_FLUSH 异步刷盘
#- SYNC_FLUSH 同步刷盘
flushDiskType=ASYNC_FLUSH
```

## 3.3 启动集群
使用命令 `docker-compose up --build -d --force-recreate` 即可，稍等一会后浏览器打开 `http://localhost:8080` 即可看到集群相关信息（页面工具稍后介绍）:

![](https://raw.githubusercontent.com/lujiahao0708/PicRepo/master/blogPic/RocketMQ/RocketMQ%E5%9F%BA%E7%A1%80/rocketmq-cluster-console.png)



# 4.集群监控管理
## 4.1 mqadmin
了解即可，参考文章 xxx
## 4.2 RocketMQ-console
`RocketMQ` 扩展的开源项目 [incubator-rocketmq-externals](https://github.com/apache/rocketmq-externals)，这个项目中有一个子模块叫 `rocketmq-console`，这个便是管理控制台项目。这个项目可以通过图形化的方式操作 RocketMQ，简单方便。

### 下载编译安装
先将 [incubator-rocketmq-externals](https://github.com/apache/rocketmq-externals) 拉到本地，因为我们需要自己对 `rocketmq-console` 进行编译打包运行。

```sh
git clone https://github.com/apache/rocketmq-externals
cd rocketmq-console
mvn clean package -Dmaven.test.skip=true
```

注意：打包前在 `rocketmq-console` 中配置 `namesrv` 集群地址：

```sh
application.properties 中修改
rocketmq.config.namesrvAddr=192.168.25.135:9876;192.168.25.138:9876
```

打包好后上传到服务器中，启动 rocketmq-console：

```sh
java -jar rocketmq-console-ng-1.0.0.jar
```

启动成功后，我们就可以通过浏览器访问 `http://localhost:8080` 进入控制台界面查看集群信息。

### Docker 安装(推荐)
镜像地址： [styletang/rocketmq-console-ng](https://hub.docker.com/r/styletang/rocketmq-console-ng)，参考 `docker-compose.yml` 文件中的配置。
